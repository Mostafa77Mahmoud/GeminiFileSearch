import streamlit as st
import requests
import json
import os
from datetime import datetime
from typing import Optional, Dict, Any, Tuple
from config import Config

st.set_page_config(
    page_title="ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆØ¯ - Gemini File Search",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Use 127.0.0.1 for internal connection
API_BASE_URL = "http://127.0.0.1:{}".format(Config.FLASK_PORT)
RESULTS_DIR = "results"

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
os.makedirs(RESULTS_DIR, exist_ok=True)

def check_api_health() -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Flask API"""
    try:
        response = requests.get("{}/health".format(API_BASE_URL), timeout=2)
        return response.status_code == 200
    except:
        return False

def get_store_info() -> Optional[Dict[str, Any]]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª File Search Store"""
    try:
        response = requests.get("{}/store-info".format(API_BASE_URL), timeout=5)
        if response.status_code == 200:
            return response.json()
        return None
    except:
        return None

def file_search_request(contract_text: str, top_k: int = 10) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    """Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ù€ API"""
    try:
        response = requests.post(
            "{}/file_search".format(API_BASE_URL),
            json={"contract_text": contract_text, "top_k": top_k},
            timeout=300  # Ø²ÙŠØ§Ø¯Ø© timeout Ø¥Ù„Ù‰ 5 Ø¯Ù‚Ø§Ø¦Ù‚
        )
        
        if response.status_code == 200:
            return response.json(), None
        else:
            return None, response.json().get("error", "Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")
    except Exception as e:
        return None, str(e)

def save_results_to_file(result: Dict[str, Any], contract_text: str) -> str:
    """Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„Ù JSON"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = "analysis_{}.json".format(timestamp)
    filepath = os.path.join(RESULTS_DIR, filename)
    
    # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    output_data = {
        "timestamp": timestamp,
        "contract_length": len(contract_text),
        "total_chunks": result.get("total_chunks", 0),
        "extracted_terms": result.get("extracted_terms", []),
        "chunks": result.get("chunks", [])
    }
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    return filepath

# ============= Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© =============
st.title("âš–ï¸ Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆØ¯ - Gemini File Search")
st.markdown("### Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆÙÙ‚ Ù…Ø¹Ø§ÙŠÙŠØ± AAOIFI")

# Sidebar
with st.sidebar:
    st.header("ğŸ“‹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
    
    if check_api_health():
        st.success("âœ… Ø§Ù„Ù€ API ÙŠØ¹Ù…Ù„")
    else:
        st.error("âŒ Ø§Ù„Ù€ API ØºÙŠØ± Ù…ØªØ§Ø­")
        st.stop()
    
    store_info = get_store_info()
    if store_info:
        st.subheader("ğŸ“Š File Search Store")
        st.metric("Ø§Ù„Ø­Ø§Ù„Ø©", store_info.get('status', 'unknown'))
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª", "1 (AAOIFI Reference)")
    
    st.divider()
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    results_count = len([f for f in os.listdir(RESULTS_DIR) if f.endswith('.json')]) if os.path.exists(RESULTS_DIR) else 0
    st.metric("Ø¹Ø¯Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©", results_count)
    
    st.info("ğŸ’¾ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙŠØªÙ… Ø­ÙØ¸Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙÙŠ Ù…Ø¬Ù„Ø¯ `results/`")

# Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
st.markdown("---")

# Ù‚Ø³Ù… Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
st.header("ğŸ” Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø¯")
st.markdown("Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ø¹Ù‚Ø¯ ÙˆØ§ØªØ±ÙƒÙ†Ø§ Ù†Ø­Ù„Ù„Ù‡ ÙˆÙÙ‚ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø´Ø±ÙŠØ¹Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©")

# Ø´Ø±ÙŠØ· Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
contract_input = st.text_area(
    "ğŸ“„ Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ø¹Ù‚Ø¯:",
    height=250,
    placeholder="Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ù‡Ù†Ø§...",
    label_visibility="visible"
)

# Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    run_search = st.button("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„", type="primary", use_container_width=True)
with col2:
    top_k = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", min_value=5, max_value=50, value=10)
with col3:
    st.empty()

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù‡Ù…Ø©
st.info("â„¹ï¸ **Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ØªØ³ØªØºØ±Ù‚ 2-4 Ø¯Ù‚Ø§Ø¦Ù‚ Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù†Øµ. ÙŠØªÙ… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù„Ù‰ Ù…Ø±Ø­Ù„ØªÙŠÙ†:\n"
        "1ï¸âƒ£ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ù†ÙˆØ¯ Ø§Ù„Ù…Ù‡Ù…Ø©\n"
        "2ï¸âƒ£ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† (Ø¹Ø§Ù… + Ù…Ø¹Ù…Ù‘Ù‚ Ù„Ù„Ø¨Ù†ÙˆØ¯ Ø§Ù„Ø­Ø³Ø§Ø³Ø©)")

st.markdown("---")

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨
if run_search:
    if not contract_input.strip():
        st.error("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø§Ù„Ø¹Ù‚Ø¯ Ø£ÙˆÙ„Ø§Ù‹")
    else:
        # Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar = st.progress(0)
        status_container = st.container()
        
        with status_container:
            with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„... (Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ 2-4 Ø¯Ù‚Ø§Ø¦Ù‚)"):
                result, error = file_search_request(contract_input, int(top_k))
                progress_bar.progress(100)
        
        if error:
            st.error("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {}".format(error))
        elif result:
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            result_path = save_results_to_file(result, contract_input)
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            st.success("âœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
            
            # Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ“ Ø¹Ø¯Ø¯ Ø§Ù„Ø¨Ù†ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©", len(result.get("extracted_terms", [])))
            with col2:
                st.metric("ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù€ Chunks", result.get("total_chunks", 0))
            with col3:
                st.metric("ğŸ’¾ ØªÙ… Ø§Ù„Ø­ÙØ¸", os.path.basename(result_path))
            
            st.markdown("---")
            
            # Ù‚Ø³Ù… Ø§Ù„Ø¨Ù†ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
            if result.get("extracted_terms"):
                st.subheader("ğŸ” Ø§Ù„Ø¨Ù†ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯")
                
                with st.expander("Ø¹Ø±Ø¶ Ø§Ù„Ø¨Ù†ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©", expanded=False):
                    for idx, term in enumerate(result.get("extracted_terms", []), 1):
                        with st.container():
                            col1, col2 = st.columns([1, 4])
                            with col1:
                                st.write(f"**Ø§Ù„Ø¨Ù†Ø¯ #{idx}**")
                            with col2:
                                st.write(f"**{term.get('term_id', 'N/A')}**")
                            
                            st.write(f"ğŸ“Œ **Ø§Ù„Ù†Øµ:** {term.get('term_text', '')[:200]}...")
                            
                            issues = term.get('potential_issues', [])
                            if issues:
                                st.write(f"âš ï¸ **Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:** {', '.join(issues)}")
                            
                            st.write(f"ğŸ’¡ **Ø§Ù„Ø³Ø¨Ø¨:** {term.get('relevance_reason', '')}")
                            st.divider()
            
            # Ù‚Ø³Ù… Ø§Ù„Ù€ Chunks
            st.subheader("ğŸ“¦ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ù…Ø¹Ø§ÙŠÙŠØ± AAOIFI")
            
            if result.get('chunks'):
                # Ø´Ø±ÙŠØ· Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ù€ chunks
                search_query = st.text_input("ğŸ” Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:", placeholder="Ø§Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø©...")
                
                # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                chunks = result.get('chunks', [])
                if search_query:
                    chunks = [c for c in chunks if search_query.lower() in c.get('chunk_text', '').lower()]
                
                st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: **{len(chunks)}** Ù…Ù† **{result.get('total_chunks', 0)}**")
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù€ chunks
                for idx, chunk in enumerate(chunks, 1):
                    with st.container():
                        # Ø±Ø£Ø³ Ø§Ù„Ù€ chunk
                        col1, col2, col3 = st.columns([2, 1, 1])
                        with col1:
                            st.write(f"**ğŸ“‹ Chunk #{idx}**")
                        with col2:
                            score = chunk.get('score', 0)
                            st.metric("Ø§Ù„ØµÙ„Ø©", f"{score:.2%}")
                        with col3:
                            st.write(f"**{len(chunk.get('chunk_text', ''))} Ø­Ø±Ù**")
                        
                        # Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù€ chunk
                        chunk_text = chunk.get('chunk_text', '')
                        # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 300 Ø­Ø±Ù Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹
                        if len(chunk_text) > 300:
                            with st.expander("Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„"):
                                st.write(chunk_text)
                            st.write(chunk_text[:300] + "...")
                        else:
                            st.write(chunk_text)
                        
                        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±
                        if chunk.get('uri'):
                            st.caption(f"ğŸ“‚ Ø§Ù„Ù…ØµØ¯Ø±: {chunk.get('uri', 'N/A')}")
                        
                        st.divider()
                
                # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
                st.markdown("---")
                col1, col2 = st.columns(2)
                
                with col1:
                    # ØªØ­Ù…ÙŠÙ„ JSON
                    json_data = json.dumps(result, ensure_ascii=False, indent=2)
                    st.download_button(
                        label="â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (JSON)",
                        data=json_data,
                        file_name="analysis_{}.json".format(datetime.now().strftime("%Y%m%d_%H%M%S")),
                        mime="application/json",
                        use_container_width=True
                    )
                
                with col2:
                    # ØªØ­Ù…ÙŠÙ„ Ù†Øµ Ø¹Ø§Ø¯ÙŠ
                    text_output = "=== Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø¯ ===\n\n"
                    text_output += f"Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now()}\n"
                    text_output += f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¨Ù†ÙˆØ¯: {len(result.get('extracted_terms', []))}\n"
                    text_output += f"Ø¹Ø¯Ø¯ Ø§Ù„Ù€ Chunks: {result.get('total_chunks', 0)}\n\n"
                    
                    text_output += "--- Ø§Ù„Ø¨Ù†ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© ---\n"
                    for term in result.get('extracted_terms', []):
                        text_output += f"\n{term.get('term_id')}: {term.get('term_text')}\n"
                    
                    st.download_button(
                        label="ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Text)",
                        data=text_output,
                        file_name="analysis_{}.txt".format(datetime.now().strftime("%Y%m%d_%H%M%S")),
                        mime="text/plain",
                        use_container_width=True
                    )
            else:
                st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")

# Ù‚Ø³Ù… Ø§Ù„Ø³Ø¬Ù„
st.markdown("---")
st.header("ğŸ“œ Ø§Ù„Ø³Ø¬Ù„")

if os.path.exists(RESULTS_DIR):
    result_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.endswith('.json')], reverse=True)
    
    if result_files:
        st.subheader("Ø¢Ø®Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª")
        
        with st.expander("Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„", expanded=False):
            for filename in result_files[:10]:  # Ø¹Ø±Ø¶ Ø¢Ø®Ø± 10 Ù†ØªØ§Ø¦Ø¬
                st.write(f"ğŸ“ {filename}")
                filepath = os.path.join(RESULTS_DIR, filename)
                
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.caption(f"Ø§Ù„ØªØ§Ø±ÙŠØ®: {data.get('timestamp', 'N/A')}")
                    with col2:
                        st.caption(f"Ø§Ù„Ø¨Ù†ÙˆØ¯: {len(data.get('extracted_terms', []))}")
                    with col3:
                        st.caption(f"Chunks: {data.get('total_chunks', 0)}")
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ø³Ø¨Ù‚Ø©")
